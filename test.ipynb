{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc8b625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  search_query                                              title  \\\n",
      "0    도산근린공원 후기                 도산공원 강남 공원 서울 도심 숨겨진 명소 도산근린공원을 가다   \n",
      "1    도산근린공원 후기               압구정 도산공원 산책로 서울 역사탐방 도산 안창호 기념관 관람후기   \n",
      "2    도산근린공원 후기  [서울 강남구] 걸음은 가볍지만 마음은 무거웠던 도산 공원 산보 후기 (feat. ...   \n",
      "3    도산근린공원 후기                            압구정 도산공원 처음 방문하여 산책한 후기   \n",
      "4    도산근린공원 리뷰                                압구정, 어디까지 가봤니? 도산공원   \n",
      "\n",
      "                                             content                 date  \\\n",
      "0  도산근린공원\\r\\n서울 도심의 숨겨진 명소\\r\\n도산공원 나들이\\r\\n복잡한 서울 ...   2023. 4. 14. 18:09   \n",
      "1  도산안창호선생기념사업회\\r\\n도산 안창호 기념관\\r\\n도산근린공원\\r\\n✔️ 관람장...    2024. 3. 8. 19:35   \n",
      "2  안녕하세요.\\r\\n호림 박물관 신사 분관을\\r\\n관람하고 나서니\\r\\n도산공원 비석...        2023. 12. 19.   \n",
      "3  방문 계기 및 위치\\r\\n50m\\r\\n© NAVER Corp.\\r\\n도산공원\\r\\n...   2023. 10. 3. 23:29   \n",
      "4  안녕하세요! 공원 고인물 존입니다\\r\\n오늘은 압구정 로데오거리 근처에 있는 산책하...  2021. 11. 27. 20:45   \n",
      "\n",
      "                                            blog_url park_name  \n",
      "0  https://blog.naver.com/homilbbangbooboo/223074...    도산근린공원  \n",
      "1      https://blog.naver.com/uniqueuni/223377455092    도산근린공원  \n",
      "2  https://blog.naver.com/8-dofourseasons/2235786...    도산근린공원  \n",
      "3  https://blog.naver.com/yellowblueblack/2232273...    도산근린공원  \n",
      "4      https://blog.naver.com/chxndygks/222580394907    도산근린공원  \n",
      "Index(['search_query', 'title', 'content', 'date', 'blog_url', 'park_name'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./crawling/naver_blog_reviews_removed_final.csv\")\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf6ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치, 중복 제거\n",
    "df = df.dropna(subset=[\"content\"])\n",
    "df = df.drop_duplicates(subset=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4feab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식으로 광고성 문구 제거\n",
    "import re\n",
    "\n",
    "ad_patterns = [\n",
    "    r\"협찬\", r\"광고\", r\"서포터즈\", r\"제공.?받아\", r\"이웃추가\", \n",
    "    r\"좋아요\", r\"더보기\", r\"클릭\", r\"후기입니다\"\n",
    "]\n",
    "\n",
    "pattern = \"|\".join(ad_patterns)\n",
    "df[\"content\"] = df[\"content\"].apply(lambda x: re.sub(pattern, \"\", str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "619b88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 태그, 특수문자, 불필요한 공백 제거\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    # HTML 태그 제거\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # 이모지 제거\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A3\\u3130-\\u318F\\s.,!?]\", \" \", text)\n",
    "    # 다중 공백 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"content\"] = df[\"content\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b288833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좋아요가 들어가지만 긍정의 의미가 아닌\n",
    "# 금지어/긍정오류 방지 처리\n",
    "ban_phrases = [\"좋아요 버튼\", \"구독\", \"팔로우\", \"이벤트 참여\"]\n",
    "for phrase in ban_phrases:\n",
    "    df[\"content\"] = df[\"content\"].str.replace(phrase, \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7410d007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://github.com/hyunwoongko/python-mecab-kor\n",
      "- konlpy.tag.Mecab: https://konlpy.org/en/latest/api/konlpy.tag/#mecab-class\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m kss\u001b[38;5;241m.\u001b[39msplit_sentences(review):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sent) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m:  \u001b[38;5;66;03m# 5글자 이상만\u001b[39;00m\n\u001b[1;32m      8\u001b[0m             sentences\u001b[38;5;241m.\u001b[39mappend(sent)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kss/_modules/sentences/split_sentences.py:89\u001b[0m, in \u001b[0;36msplit_sentences\u001b[0;34m(text, backend, num_workers, strip, return_morphemes, ignores)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     split_fn \u001b[38;5;241m=\u001b[39m _split_sentences\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _run_job(\n\u001b[1;32m     90\u001b[0m     func\u001b[38;5;241m=\u001b[39mpartial(\n\u001b[1;32m     91\u001b[0m         split_fn,\n\u001b[1;32m     92\u001b[0m         backend\u001b[38;5;241m=\u001b[39mbackend_analyzer,\n\u001b[1;32m     93\u001b[0m         strip\u001b[38;5;241m=\u001b[39mstrip,\n\u001b[1;32m     94\u001b[0m         return_morphemes\u001b[38;5;241m=\u001b[39mreturn_morphemes,\n\u001b[1;32m     95\u001b[0m         preprocessor\u001b[38;5;241m=\u001b[39m_preprocessor,\n\u001b[1;32m     96\u001b[0m         postprocessor\u001b[38;5;241m=\u001b[39m_postprocessor,\n\u001b[1;32m     97\u001b[0m     ),\n\u001b[1;32m     98\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m     99\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m    100\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kss/_utils/multiprocessing.py:27\u001b[0m, in \u001b[0;36m_run_job\u001b[0;34m(func, inputs, num_workers)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m         output \u001b[38;5;241m=\u001b[39m func(inputs)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         output \u001b[38;5;241m=\u001b[39m [func(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inputs]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kss/_modules/sentences/split_sentences.py:136\u001b[0m, in \u001b[0;36m_split_sentences\u001b[0;34m(text, backend, strip, postprocess, recursion, return_morphemes, preprocessor, postprocessor)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    135\u001b[0m     backup_sentence \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mbackup(text)\n\u001b[0;32m--> 136\u001b[0m     morphemes \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mpos(backup_sentence, drop_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    137\u001b[0m     syllables \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mpreprocess(morphemes)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], Syllable):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/kss/_modules/morphemes/analyzers.py:64\u001b[0m, in \u001b[0;36mPecabAnalyzer.pos\u001b[0;34m(self, text, drop_space)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, drop_space: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    Get pos information.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m        List[Tuple[str, str]]: output of analysis.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_analyzer\u001b[38;5;241m.\u001b[39mpos(text)\n\u001b[1;32m     65\u001b[0m     output \u001b[38;5;241m=\u001b[39m _preserve_space(text, output, spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\v\u001b[39;00m\u001b[38;5;130;01m\\f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_space:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pecab/_pecab.py:45\u001b[0m, in \u001b[0;36mPeCab.pos\u001b[0;34m(self, text, drop_space)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, drop_space: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 45\u001b[0m     tokenization_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(text)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     47\u001b[0m         (token, pos)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token, pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (drop_space \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\f\u001b[39;00m\u001b[38;5;130;01m\\v\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pecab/_pecab.py:19\u001b[0m, in \u001b[0;36mPeCab._tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mset_input(text)\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mincrement_token():\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     token_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mtoken_attributes\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pecab/_tokenizer.py:292\u001b[0m, in \u001b[0;36mTokenizer.increment_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend:\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m    294\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpending\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    295\u001b[0m length \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mlength\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pecab/_tokenizer.py:384\u001b[0m, in \u001b[0;36mTokenizer.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_entries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_entries\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 384\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    385\u001b[0m             surface,\n\u001b[1;32m    386\u001b[0m             data_entry,\n\u001b[1;32m    387\u001b[0m             pos_data,\n\u001b[1;32m    388\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos,\n\u001b[1;32m    389\u001b[0m             pos_ahead \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    390\u001b[0m             Type\u001b[38;5;241m.\u001b[39mKNOWN,\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    392\u001b[0m         any_matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    394\u001b[0m pos_ahead \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pecab/_tokenizer.py:275\u001b[0m, in \u001b[0;36mTokenizer.add\u001b[0;34m(self, surface, data_dict, from_pos_data, word_pos, end_pos, type_)\u001b[0m\n\u001b[1;32m    272\u001b[0m         least_idx \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m    274\u001b[0m least_cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m word_cost\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions\u001b[38;5;241m.\u001b[39mget(end_pos)\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    276\u001b[0m     cost\u001b[38;5;241m=\u001b[39mleast_cost,\n\u001b[1;32m    277\u001b[0m     last_right_id\u001b[38;5;241m=\u001b[39mright_id,\n\u001b[1;32m    278\u001b[0m     back_pos\u001b[38;5;241m=\u001b[39mfrom_pos_data\u001b[38;5;241m.\u001b[39mpos,\n\u001b[1;32m    279\u001b[0m     back_rpos\u001b[38;5;241m=\u001b[39mword_pos,\n\u001b[1;32m    280\u001b[0m     back_index\u001b[38;5;241m=\u001b[39mleast_idx,\n\u001b[1;32m    281\u001b[0m     back_id\u001b[38;5;241m=\u001b[39mword_id,\n\u001b[1;32m    282\u001b[0m     back_dict_type\u001b[38;5;241m=\u001b[39mtype_,\n\u001b[1;32m    283\u001b[0m     back_pos_type\u001b[38;5;241m=\u001b[39mback_pos_type,\n\u001b[1;32m    284\u001b[0m     morphemes\u001b[38;5;241m=\u001b[39mmorphemes,\n\u001b[1;32m    285\u001b[0m     back_pos_tag\u001b[38;5;241m=\u001b[39mleft_pos,\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pecab/_tokenizer.py:179\u001b[0m, in \u001b[0;36mTokenizer.WrappedPositionArray.get\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_positions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_positions\u001b[38;5;241m.\u001b[39mappend(Tokenizer\u001b[38;5;241m.\u001b[39mPosition())\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_positions[\n\u001b[1;32m    182\u001b[0m     : \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write\n\u001b[1;32m    183\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions[\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write : \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write\n\u001b[1;32m    185\u001b[0m ]\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_positions[\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write\n\u001b[1;32m    188\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 문장 분리 -> 분석 대상 문장만 남기기\n",
    "import kss  # 한국어 문장분리기\n",
    "\n",
    "sentences = []\n",
    "for review in df[\"content\"]:\n",
    "    for sent in kss.split_sentences(review):\n",
    "        if len(sent) >= 5:  # 5글자 이상만\n",
    "            sentences.append(sent)\n",
    "df_sent = pd.DataFrame(sentences, columns=[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46ce2f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Downloading jpype1-1.6.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from konlpy) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/anaconda3/lib/python3.12/site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
      "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-macosx_10_9_universal2.whl (582 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.2/582.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.6.0 konlpy-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석, 불용어 제거\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "stopwords = [\"이\", \"그\", \"저\", \"그리고\", \"하지만\", \"또한\"]\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = okt.morphs(text, stem=True)\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df_sent[\"tokens\"] = df_sent[\"sentence\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 데이터 저장\n",
    "df_sent.to_csv(\"test_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
