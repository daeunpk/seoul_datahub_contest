import os
import pandas as pd
from transformers import pipeline
from tqdm import tqdm

import torch
from transformers import BertTokenizer, BertForSequenceClassification
import torch.nn.functional as F

# --- 1. ì´ˆê¸° ì„¤ì • ---

# ğŸ”½ 1-1. ì§ì ‘ í™•ì¸í•˜ê³  ìì—° ê´€ë ¨ í† í”½ë§Œ ë‚¨ê²¨ë‘” í†µí•© íŒŒì¼ì˜ ê²½ë¡œ
MANUALLY_FILTERED_TOPICS_PATH = "./bertopic_results/combine_topic_summary_cleaned.csv" 

# ğŸ”½ 1-2. ê³µì›ë³„ ë¬¸ì„œ-í† í”½ íŒŒì¼ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ
DOCS_TOPICS_DIR = "./bertopic_results/"

# ğŸ”½ 1-3. ìµœì¢… ê²°ê³¼ê°€ ì €ì¥ë  íŒŒì¼ ì´ë¦„
FINAL_SCORE_OUTPUT_PATH = "ê°ì •ë¶„ì„_test.csv"

# --- 2. ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë”© (ì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ ë§¨ ì²˜ìŒì— í•œ ë²ˆë§Œ ì‹¤í–‰) ---
print("ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
# GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ device=0ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì†ë„ê°€ ë§¤ìš° ë¹¨ë¼ì§‘ë‹ˆë‹¤. (ì—†ìœ¼ë©´ device=-1 ë˜ëŠ” ìƒëµ)
sentiment_analyzer = pipeline('sentiment-analysis', model='kykim/bert-kor-base', device=-1)
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")


# --- 3. ë¶„ì„ ì‹œì‘ ---

try:
    # 3-1. ìˆ˜ë™ìœ¼ë¡œ í•„í„°ë§í•œ 'ìì—° ê´€ë ¨' í† í”½ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    approved_topics_df = pd.read_csv(MANUALLY_FILTERED_TOPICS_PATH)
    # í˜¹ì‹œ ëª¨ë¥¼ -1 í† í”½(ë…¸ì´ì¦ˆ)ì€ ì½”ë“œ ìƒì—ì„œ í•œ ë²ˆ ë” ì œì™¸
    approved_topics_df = approved_topics_df[approved_topics_df['Topic'] != -1]
    print(f"\nâœ… '{MANUALLY_FILTERED_TOPICS_PATH}' íŒŒì¼ì—ì„œ {len(approved_topics_df)}ê°œì˜ ìì—° ê´€ë ¨ í† í”½ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

except FileNotFoundError:
    print(f"âŒ ì˜¤ë¥˜: '{MANUALLY_FILTERED_TOPICS_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    approved_topics_df = pd.DataFrame() # ë¹ˆ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì´ˆê¸°í™”

# 3-2. ê³µì›ë³„ ë¬¸ì„œ-í† í”½ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
try:
    doc_topic_files = [f for f in os.listdir(DOCS_TOPICS_DIR) if f.endswith("_document_topics.csv")]
    print(f"âœ… '{DOCS_TOPICS_DIR}' í´ë”ì—ì„œ {len(doc_topic_files)}ê°œì˜ ê³µì› ë¦¬ë·° íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
except FileNotFoundError:
    print(f"âŒ ì˜¤ë¥˜: '{DOCS_TOPICS_DIR}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    doc_topic_files = []

# --- 4. ê³µì›ë³„ ì ìˆ˜ ê³„ì‚° ë£¨í”„ ---

final_park_scores = {}

if not approved_topics_df.empty and doc_topic_files:
    for filename in tqdm(doc_topic_files, desc="ê³µì›ë³„ ê°ì„± ì ìˆ˜ ê³„ì‚° ì¤‘"):
        
        # 4-1. íŒŒì¼ëª…ì—ì„œ ê³µì› ì´ë¦„ ì¶”ì¶œ
        park_name = filename.replace("_document_topics.csv", "")
        
        # 4-2. í•´ë‹¹ ê³µì›ì˜ 'ìì—° ê´€ë ¨' í† í”½ ID ëª©ë¡ë§Œ ê°€ì ¸ì˜¤ê¸°
        approved_ids_for_park = approved_topics_df[approved_topics_df['Park_nm'] == park_name]['Topic'].tolist()
        
        if not approved_ids_for_park:
            # print(f"INFO: '{park_name}'ì— í•´ë‹¹í•˜ëŠ” ìì—° ê´€ë ¨ í† í”½ì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # 4-3. í•´ë‹¹ ê³µì›ì˜ ì „ì²´ ë¦¬ë·° íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
        file_path = os.path.join(DOCS_TOPICS_DIR, filename)
        park_reviews_df = pd.read_csv(file_path)
        
        # 4-4. 'ìì—° ê´€ë ¨' í† í”½ì— í•´ë‹¹í•˜ëŠ” ë¦¬ë·°ë§Œ í•„í„°ë§
        eco_reviews_df = park_reviews_df[park_reviews_df['Topic_ID'].isin(approved_ids_for_park)]
        
        if eco_reviews_df.empty:
            # print(f"INFO: '{park_name}'ì— ìì—° ê´€ë ¨ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        # 4-5. í•„í„°ë§ëœ ë¦¬ë·°ë“¤ì— ëŒ€í•´ ê°ì„± ë¶„ì„ ìˆ˜í–‰
        reviews_to_analyze = eco_reviews_df['Review'].dropna().astype(str).tolist()
        if not reviews_to_analyze:
            continue
            
        sentiment_results = sentiment_analyzer(reviews_to_analyze)
        
        # 4-6. ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜ (ê¸ì •=1, ë¶€ì •=-1)
        scores = []
        for result in sentiment_results:
            if result['label'] == 'positive':
                scores.append(1)
            elif result['label'] == 'negative':
                scores.append(-1)
            # beomi/kcbert-base-finetune-nsmc ëª¨ë¸ì€ ì¤‘ë¦½(neutral)ì„ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        
        # 4-7. ê³µì›ì˜ ìµœì¢… 'ìƒíƒœê²½ê´€ì„± ì ìˆ˜' ê³„ì‚°
        if scores:
            final_score = sum(scores) / len(scores)
            final_park_scores[park_name] = {
                'Ecological_Scenic_Score': final_score,
                'Analyzed_Review_Count': len(scores)
            }

# --- 5. ìµœì¢… ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥ ---

if final_park_scores:
    # ë”•ì…”ë„ˆë¦¬ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    final_scores_df = pd.DataFrame.from_dict(final_park_scores, orient='index')
    final_scores_df.index.name = 'Park_nm'
    final_scores_df.reset_index(inplace=True)
    
    # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    final_scores_df_sorted = final_scores_df.sort_values(by='Ecological_Scenic_Score', ascending=False)
    
    print("\n" + "="*80)
    print("      ğŸŒ³ ìµœì¢… ê³µì›ë³„ ìƒíƒœê²½ê´€ì„± ì ìˆ˜ ìˆœìœ„ ğŸŒ³")
    print("="*80)
    print(final_scores_df_sorted.to_string(index=False))
    
    # ìµœì¢… ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    final_scores_df_sorted.to_csv(FINAL_SCORE_OUTPUT_PATH, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ìµœì¢… ê²°ê³¼ê°€ '{FINAL_SCORE_OUTPUT_PATH}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("\në¶„ì„í•  ë°ì´í„°ê°€ ì—†ì–´ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œì™€ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")