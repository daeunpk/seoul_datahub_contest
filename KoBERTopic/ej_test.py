import os
import subprocess
import pandas as pd
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer
from konlpy.tag import Mecab
from bertopic import BERTopic
import stopwordsiso as stopwords

# âœ¨ IMPROVEMENT: ëª¨ë“  ë¡œì§ì„ main í•¨ìˆ˜ë¡œ ë¬¶ì–´ ì½”ë“œ êµ¬ì¡° ê°œì„ 
def main():
    """
    ë©”ì¸ í† í”½ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜
    """
    # --- 1. ë¶ˆìš©ì–´ ì¤€ë¹„ ---
    print("1. ë¶ˆìš©ì–´ ëª©ë¡ì„ ì¤€ë¹„í•©ë‹ˆë‹¤...")
    stopwords_ko_base = stopwords.stopwords("ko")
    extra_stopwords = {
        "ì•„ìš”", "ì–´ìš”", "ì…ë‹ˆë‹¤", "ì˜ˆìš”", "ë„¤ìš”", "ê°™ì•„ìš”", "ê±°ì—ìš”", "ê±°ì˜ˆìš”", "ìŠµë‹ˆë‹¤",
        "ê·¸ë¦¬ê³ ", "ê°€ì¥", "ì •ë§", "ì§„ì§œ", "ë„ˆë¬´", "ëŠ”ë°", "ì–´ì„œ", "ë‹¤ëŠ”", "ì´ë ‡ê²Œ",
        "ì—¬ì†Œ", "ë‹¬ë¦¬", "^^", "~^^", "ê±°ë‚˜", "í•©ë‹ˆë‹¤", "ë¡œìš´", "ë¼ë¦¬", "ë©´ì„œ",
        "to", "1988", "88", "ë§ˆë‹¤", "ì§€ìš”", "ì¤‘êµ­", "ì‹œí‚¬", "ì•„ë‹Œ", "í•œë•Œ",
        "ì‹ ì˜ë³µ", "ë“ ë‹¤", "ì›í•˜", "ë¹„ì¶•", "ê¸°ì§€", "ë§ˆì‹œ", "ë©´ì„œ", "ë°©ì‚¬ì¥", "ì–´ì„œ",
        "ë‹¤ë©´", "ë‚˜ê°ˆ", "í„°ë„", "ì´ëŸ¬", "êµ°ìš”", "ì•„ì„œ", "01", "ìœ¼ëŸ¬", "ì¸ë°",
        "ì€ë°", "ì•„ì£¼"
    }
    domain_stopwords = {"í•œê°•", "ê³µì›", "ì„œìš¸"}
    final_stopwords = stopwords_ko_base.union(extra_stopwords).union(domain_stopwords)
    print(f"   - ì´ {len(final_stopwords)}ê°œì˜ ë¶ˆìš©ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # --- 2. í† í¬ë‚˜ì´ì € ë° ë²¡í„°ë¼ì´ì € ì¤€ë¹„ ---
    print("2. ì‚¬ìš©ì ì •ì˜ í† í¬ë‚˜ì´ì €ì™€ ë²¡í„°ë¼ì´ì €ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤...")

    # âœ¨ IMPROVEMENT: í•˜ë“œì½”ë”©ëœ ê²½ë¡œ ëŒ€ì‹  ë™ì ìœ¼ë¡œ ê²½ë¡œ ì°¾ê¸°
    try:
        mecab_path_process = subprocess.run(
            ["bash", "-c", "echo $(brew --prefix)/lib/mecab/dic/mecab-ko-dic"],
            capture_output=True, text=True, check=True
        )
        mecab_dic_path = mecab_path_process.stdout.strip()
        print(f"   - MeCab ì‚¬ì „ ê²½ë¡œë¥¼ ì„±ê³µì ìœ¼ë¡œ ì°¾ì•˜ìŠµë‹ˆë‹¤: {mecab_dic_path}")
    except Exception:
        print("   - âš ï¸ MeCab ì‚¬ì „ ê²½ë¡œë¥¼ ìë™ìœ¼ë¡œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        mecab_dic_path = "/opt/homebrew/lib/mecab/dic/mecab-ko-dic" # Fallback ê²½ë¡œ

    class CustomTokenizer:
        def __init__(self, tagger, stopwords):
            self.tagger = tagger
            self.stopwords = stopwords
            # ğŸ› BUG FIX: í—ˆìš©í•  í’ˆì‚¬(POS) ë¦¬ìŠ¤íŠ¸ë¥¼ __init__ì— ì •ì˜
            # ëª…ì‚¬(NNG, NNP), ë™ì‚¬(VV), í˜•ìš©ì‚¬(VA)ë¥¼ ì£¼ë¡œ ì‚¬ìš©
            self.allowed_pos = {'NNG', 'NNP', 'VV', 'VA'}

        def __call__(self, text):
            tokens = []
            for word, pos in self.tagger.pos(text):
                # ê¸¸ì´ê°€ 1 ì´ìƒì´ê³ , í—ˆìš©ëœ í’ˆì‚¬ì´ë©´ì„œ, ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ ë‹¨ì–´ë§Œ ì¶”ì¶œ
                if len(word) > 1 and pos in self.allowed_pos and word not in self.stopwords:
                    tokens.append(word)
            return tokens

    # ğŸ› BUG FIX: ì¼ë¶€ ë¶ˆìš©ì–´(extra_stopwords)ê°€ ì•„ë‹Œ ì „ì²´ ë¶ˆìš©ì–´(final_stopwords) ì‚¬ìš©
    tokenizer = CustomTokenizer(Mecab(dicpath=mecab_dic_path), stopwords=final_stopwords)

    vectorizer = CountVectorizer(tokenizer=tokenizer,
                                 max_features=5000,
                                 min_df=5,
                                 max_df=0.5)

    # --- 3. BERTopic ëª¨ë¸ ì¤€ë¹„ ---
    print("3. BERTopic ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    model = BERTopic(
        embedding_model="sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens",
        vectorizer_model=vectorizer,
        nr_topics=50,
        top_n_words=10,
        calculate_probabilities=True,
        verbose=True
    )

    # --- 4. ë°ì´í„° ë¡œë“œ ---
    print("4. ë¦¬ë·° ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    # âœ¨ IMPROVEMENT: ì—¬ëŸ¬ CSV íŒŒì¼ì„ ì½ëŠ” ë¡œì§ì„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬ (ì£¼ì„ ì²˜ë¦¬)
    # data_dir = "../measure/NAT/data/"
    # docs = load_all_reviews(data_dir)
    
    # ë‹¨ì¼ íŒŒì¼ ë¡œë“œ
    try:
        df = pd.read_csv('measure/NAT/data/ê°•ë‚¨_ìœ¨í˜„ê³µì›_reviews_dic_cleaned.csv')
        df.dropna(subset=['ë‚´ìš©'], inplace=True)
        docs = df['ë‚´ìš©'].astype(str).tolist()
        print(f"   - ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì´ {len(docs)}ê°œì˜ ë¦¬ë·°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
    except FileNotFoundError:
        print("   - âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        docs = []

    # --- 5. ëª¨ë¸ í•™ìŠµ ë° ê²°ê³¼ ì¶œë ¥ ---
    if not docs:
        print("   - ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    print("\n5. BERTopic ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    topics, probs = model.fit_transform(docs)

    print("\n" + "="*80)
    print("âœ… BERTopic ëª¨ë¸ë§ ê²°ê³¼: í† í”½ ì •ë³´")
    print("="*80)
    print(model.get_topic_info())

    print("\n" + "="*80)
    print("âœ… ê° í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ")
    print("="*80)
    for topic_num in sorted(model.get_topics().keys()):
        if topic_num != -1:  # -1ë²ˆ í† í”½(ë…¸ì´ì¦ˆ)ì€ ì œì™¸
            keywords = [word for word, prob in model.get_topic(topic_num)]
            print(f"Topic {topic_num}: {keywords}")

def load_all_reviews(data_dir: str) -> list:
    """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  CSV íŒŒì¼ì—ì„œ 'ë‚´ìš©' ì»¬ëŸ¼ì„ ì½ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    all_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(".csv")]
    df_list = []
    print(f"   - ì´ {len(all_files)}ê°œì˜ CSV íŒŒì¼ì„ ì½ìŠµë‹ˆë‹¤...")
    for file in tqdm(all_files, desc="CSV íŒŒì¼ ë¡œë”© ì¤‘"):
        try:
            tmp = pd.read_csv(file)
            if "ë‚´ìš©" in tmp.columns:
                tmp.dropna(subset=['ë‚´ìš©'], inplace=True)
                df_list.append(tmp[['ë‚´ìš©']])
        except Exception as e:
            print(f"   - âš ï¸ {file} ì½ê¸° ì‹¤íŒ¨: {e}")
    
    if not df_list:
        return []
    
    df = pd.concat(df_list, ignore_index=True)
    return df['ë‚´ìš©'].astype(str).tolist()

if __name__ == "__main__":
    main()